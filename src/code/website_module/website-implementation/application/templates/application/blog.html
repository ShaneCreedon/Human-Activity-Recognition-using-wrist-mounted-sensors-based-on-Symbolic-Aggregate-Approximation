<!-- Django specific Code for CSS and Other Resources -->
{% load static %}

<!DOCTYPE html>
<!--[if lt IE 7]>      <html class="no-js lt-ie9 lt-ie8 lt-ie7"> <![endif]-->
<!--[if IE 7]>         <html class="no-js lt-ie9 lt-ie8"> <![endif]-->
<!--[if IE 8]>         <html class="no-js lt-ie9"> <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js"> <!--<![endif]-->
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="description" content="">
        <meta name="viewport" content="width=device-width, initial-scale=1">

        <link rel="stylesheet" type="text/css" href="{% static 'application/stylesheets/common/navbar.css' %}">
        <link rel="stylesheet" type="text/css" href="{% static 'application/stylesheets/blog/blog-index.css' %}">
        <title> Human Activity Recognition PPG Software </title>

    </head>
    <body>
        <!-- Navigation menu -->
        <div class="nav-bar">
            <ul class="nav-table">
                <li class="nav-item"> <a href="/"> Home </li> </a>
                <li class="nav-item"> <a href="/research"> Research </li></a>
                <li class="nav-item"> <a href="/blog"> Blog </li></a>
                <li class="nav-item"> <a href="/discussion"> Discussion </li></a>
            </ul>
        </div>

        <!-- Blog Body -->
        <div class="blog-body">

            <div class="blog-header">
                <h2 id="blogdescribingtheengineeringprocessofmy4thyearprojectfordcu">Blog: Describing the engineering process of my 4th year project for DCU.</h2> <br/>

                <h2 id="theprojectinvolvesrecognisinghumanactivityusingsaxandmachinelearningtechniques">The project involves recognising human activity using SAX and machine learning techniques.</h2> <br />
                
                <p><strong>Student: Shane Creedon</strong> <br />  <br />
                <strong>Student ID: 15337356</strong> <br />  <br />
                <strong>4th year Computer Applications and Software Engineering student in DCU</strong>  </p>
            </div>

            <div class=blog-post-title>
                <h1 id="blogpost1proposalsubmissionandupcominggoals04112018">Blog Post #1 | Proposal Submission and Upcoming Goals | 04/11/2018</h1>
            </div>

            <div class="blog-post">
                <p>The basis for the project is recognising human activity using wearable sensor technology based on 
                Symbolic Aggregate Approximation (SAX) and Machine Vision.</p>
                
                <p>After discussing the idea with Tomas in great detail, we both agreed to work together on this project as part of my 4th year project.
                With the idea in mind, I constructed the proposal documented due for both Tomas and my presentation supervisors.
                The proposal specified all the nitty-gritty details of the project and I hope gave clear insight into what we are trying to achieve.</p>
                
                <p>I sent the document to Tomas for review and discussion the idea with the project presentation supervisors (Darragh &amp; Mark)
                who all thankfully approved the idea and allowed me to move forward.</p>
                
                <p>My next primary goal is to set up Continuous Integration (CI) practices for my projects GitLab either using the GitLab
                built-in CI tools or use something like Jenkins / Teamcity.</p>
                
                <p>Week 9 requires the completion of my functional specification which needs to be ~25 pages in length describing the project
                in great detail using both easy to understand language and UML diagrams depicting the technical structure.   </p>
                
                <p>Currently it is week 7, I will aim to get these above two steps complete within the next week.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost2developmentofthefunctionalspecification1122018">Blog Post #2 | Development of the functional specification | 1/12/2018</h1>
            </div>

            <div class="blog-post">
                <p>Like most students, I have began working on my Function Specification discussing my project
                idea in great detail. Within the specification, I discuss:</p>
                
                <ul>
                <li><p>An introduction section, where I overview the project and why we are building it.</p></li>
                
                <li><p>Section two provides a <em>General Description</em> of the product/system functions.
                It is in this section I detail a high-level abstract look at the individual functions our application will perform.
                I try my best to be as thorough as possible through the document while offering visualisations of the functionality for
                a stronger conceptual understanding.
                This section also discusses the many constraints my system will face throughout its development.</p></li>
                
                <li><p>Section three will discuss all of the functions of the system in great, great detail.
                Each function will have a description, criticality overview, technical issues faced, 
                dependencies on other requirements and other details that many offer insight into the function.</p></li>
                
                <li><p>Section 4 looks at the system architecture and how the overall system will piece together.
                I offer visualisations here constructed on behalf of showing how system components tie with one another.</p></li>
                
                <li><p>Section 5 looks at the high-level design / abstract view of the system. 
                Within the section displayed are 3 Data-Flow Diagrams showing how data moves around individual components.
                There is also a minimalistic conceptual class diagram of how the application classes will interact.
                Finally, there is a use case diagram showing how a particular user may interact with the system.</p></li>
                
                <li><p>Section 6 shows the schedule of project activities over the course of the project timeline.
                I showcase a Gantt Chart diagram to display this information.</p></li>
                
                <li><p>Section 7 is a list of references I made use of when developing the project idea.</p></li>
                </ul>
                
                <p>The blog is written in <a href="https://github.com/adam-p/markdown-here/wiki/Markdown-Cheatsheet">markdown</a>.
                Markdown is a simple text-based mark-up language.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost3week12endofsemesterone18122018">Blog Post #3 | Week 12 - End of Semester One | 18/12/2018</h1>
            </div>
            <div class="blog-post">
                <p>Not much progress has been made on the project since I finished the functional specification back
                in week 7/8. Other modules have taken up my time however, I have been doing some research on the topic
                of Human Activity Recognition(HAR) and Machine Learning. In addition, i've looked into libraries
                and possible design choices I could take when developing my application.</p>
                
                <p>I came across and interesting article on how SAX (Symbolic Aggregate Approximation) can be implemented
                within Java and I may use this to aid my development within Python. Google Scholar has been helpful for
                finding Machine Learning related articles in the field. I have been looking a lot into <strong>Supervised Learning</strong>
                research papers and methods to optimise convolutional neural networks for image classification.
                Now, I will take some time off to spend with my family over christmas but once January hits, I will
                be back in the books for the exams.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost4examinationperiod0612019">Blog Post #4 | Examination Period | 06/1/2019</h1>
            </div>
            <div class="blog-post">
                <p>Happy new year to those reading and a happy christmas as well. I had been researching the project over the last
                few days and coding some small prototypes to grasp how I was going to build the proclaimed software specified in the 
                functional specification. With the DCU exams in several days time, I have decided to but the project on the back seat
                while I focus thoroughly on my exams.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost5beginningdevelopmentoftheproject26012019">Blog Post #5 | Beginning development of the project | 26/01/2019</h1>
            </div>

            <div class="blog-post">
                <p>Returning back from a week-long trip, I have started working on the project. I initially wanted to understand how
                I was going to build a desktop application using Python. I looked into several different libraries which seemed to
                offer the ability to do so. Eventually after enough research, I decided to go with PyQt5, which is based on QT. Qt is set of cross-platform C++ libraries that implement high-level APIs for accessing many aspects of modern desktop and mobile systems.</p>
                
                <p>Additionally, I looked into how to convert a <code>.py</code> file into a <code>.exe</code> file for end-users which I managed to come across
                a magical library for this exact purpose: <strong>PyInstaller</strong>. PyInstaller allows me to convert a .py file into a .exe file
                along with a <code>.spec</code> configuration file for how the .exe file is run. </p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost6symbolicaggregateapproximation28012019">Blog Post #6 | Symbolic Aggregate Approximation | 28/01/2019</h1>
            </div>
            <div class="blog-post">
                <p>The project directory structure has been created. I have created a small template website which is intended
                to host the download link for the application. In addition, the SAX python file (symbolic<em>aggregate</em>approximation.py)
                was created and coded up to convert the PPG exercise data sets into a string of characters.</p>
                
                <p>For example:
                <code>cccccaabbcbcabbddddbbabdbebebeeeebdabdbdbaa</code></p>
                
                <p>The string above is just a simple example, the real string would be FAR FAR longer, representative of an hour in time.
                We have long strings like this for all 4 activities: <em>Walk/Run/Low Resistance Bike/High Resistance Bike</em></p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost7bitmapgeneration29012019">Blog Post #7 | Bitmap Generation | 29/01/2019</h1>
            </div>

            <div class="blog-post">
                <p>With the ability to generate SAX strings from the PPG data sets, the next step was to convert these strings into images.
                To do this, I found a library online called: <code>text-to-image</code>. This library allowed me to convert a subsequence of letters
                from the SAX string into a grey-scale pixel-based image. Specifically, the images are 32 pixels in size and after generating
                all the images for each SAX string for each activity, the <strong>total number of available training images is greater than 10,000</strong>.</p>
                
                <p>This grey-scale image approach may not be the optimal approach for this problem and for convolutional neural network training but,
                I will address this with my supervisor and I will research online and find out whatever the best approach is for image classification
                in relation to the technique of Symbolic Aggregate Approximation(SAX).</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost8meetingsanddiscoveries16022019">Blog Post #8 | Meetings and Discoveries | 16/02/2019</h1>
            </div>

            <div class="blog-post">
                <p>Myself and Tomas (Supervisor) have met up several times to discuss the project conceptually. He has given me great advice and
                pushed me in the right general direction with the project. I read over <strong>Eoin Brophy's</strong> research paper on human activity recognition
                in relation to PPGs and that also helped improve my abstract view of the project. Previously, I was aiming for 32 pixels within a bitmap
                image for training, testing and validation. However, this only accumulated to 0.125 milliseconds of examination. As a result of reading Eoin's paper
                and discussing with Tomas, I modified this approach to now take on 2 entire seconds of detail per image. This amounts to 512 pixels per image.  </p>
                
                <p>Due to such a high pixel count per image, we decided to invoke the powers of <strong>Transfer Learning</strong> to ultimately improve our model's accuracy, by
                using pre-existing models to save us much training time and improve our accuracy. This technique involves training the penultimate layer of the
                convolutional neural network with our training data, and training the rest of the model with the transfer learning concept.  </p>
                
                <p>Finally, Tomas suggested I do away with the saxpy library I was using as well as the text-to-image library I was using. We decided for the sake
                of research and ad hoc project requirements that I would enter into the process of building my own SAX toolkit (library) and string to image conversion
                process.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost9normalisationproblemdiscovery16022019">Blog Post #9 | Normalisation problem discovery | 16/02/2019</h1>
            </div>

            <div class="blog-post">
                <p>Through a thorough analysis of the data, I noticed the normalisation process was modifying the data negatively. After normalisation, each SAX string would generate but the distances
                between letters would be dictated solely on the data set the PPG microvolt data came from. What we really want is this to be relative to the entire project training data body. <br />
                <strong>For Example:</strong> The walk data set would map q -> ~1600 mV while the high-resistance cycle data set would map q -> 1300.  </p>
                
                <p>This is a problem and to address it I intend to make the comparison mapping values static to the project as a whole and not to the individual data set. The amplitude of our signals are discriminative, so therefore it might be best not to normalise.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost10saximplementationbitmapdetailformachinelearning17022019">Blog Post #10 | SAX implementation &amp; Bitmap Detail for Machine Learning | 17/02/2019</h1>
            </div>

            <div class="blog-post">
                <p>I have now wrote my own SAX module and bitmap generation scheme. Among the SAX module I wrote, I now have a very strong understanding of all the
                underlying components that make it up. I also made the module flexible and simple to use and tweak settings. One can specify the range of letters
                that make up the sax conversion process in addition to the lower-bounding property that makes up the window size of a particular letter.  </p>
                
                <p>To achieve this, I took X amount of entries the user desired for the window size | E.G. 10. I average out the values of these 10 entries by summing them
                and dividing by the quantity of entries. It is this average value I use to determine what letter these 10 entries should belong to. This is a simple yet
                effective method for segmenting groups of microvolt signals into a single letter. Adjusting this property may improve our accuracy &amp; efficiency in the future
                so hence why I incorporate the feature.  </p>
                
                <p>Finally, I wrote my own bitmap module using techniques found online. I decided to map each letter to a particular RGB value: <br />
                <strong>'a': (200, 125, 50)</strong> <br />
                <strong>'b': (20, 200, 50)</strong> <br />
                <strong>'c': (100, 90, 20)</strong>  </p>
                
                <p>Previously, I used the <strong>text-to-image</strong> library to convert my sax-ified textual string into grey-scale images, however for the sake of my understanding
                and to improve the accuracy of our model, I looked into converting the images into RGB 24-bit based images. Using the library for grey-scale images also
                resulted in strange image dimensions. The size of the generated image seemed unpredictable and for our model it is practically a requirement to have a
                standardized image dimension. With writing my own bitmap module, I ensured that image sizes could only be 16x16 making up 256 pixels.  </p>
                
                <blockquote>
                    <p>Example of old grey-scale images against new 24-bit rgb images</p>
                </blockquote>
                
                <p><strong>8-bit Grey-scale bitmap</strong> <br />
                <img width=400 height=400 src="{% static 'application/resources/blog/images/grey-scale_bitmap.png' %}" alt="grey-scale-bitmap" />  </p>
                
                <p><strong>24-bit RGB bitmap</strong> <br />
                <img width=400 height=400 src="{% static 'application/resources/blog/images/example-rgb-image.png' %}" alt="rgb-bitmap" /></p>
            
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost11furtheroptionalparameters22022019">Blog Post #11 | Further Optional Parameters | 22/02/2019</h1>
            </div>

            <div class="blog-post">
                <p>To provide further optionality and further diversity among my machine learning solution, I enabled the ability for both RGB (24-bit) images and Greyscale (8-bit) images to be generated. I wrote two python scripts which allow the pixels to be mapped within an RGB octet range
                and to be mapped within a Greyscale octet range.  </p>
                
                <h4 id="whydididothis">Why did I do this?</h4>
                
                <p>I did this to profilerate the amount of parameters we can adjust when building the project and tweaking the machine learning model.
                With more parameters, the flexibility of the model rises. Through tweaking these parameters we can hopefully accentuate the validation accuracy of the model while reducing the validation loss.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost12theconvolutionalneuralnetworkviatensorflowandkeras30022019">Blog Post #12 | The Convolutional Neural Network via TensorFlow and Keras | 30/02/2019</h1>
            </div>

            <div class="blog-post">
                <p>Deciding to jump right into the deep learning was the next move on the project agenda. I took a few hours to research several
                python libraries and frameworks relating to machine learning like TensorFlow or Keras, Sci-Py was another one I researched.
                Eventually I decided to embark on this data science journey using TensorFlow with Keras to construct a convolutional neural network.</p>
                
                <p>I wrote about 200 lines of python code which constructed a machine learning model, which has 15 iterations (Epochs). <br />
                I intend to use <code>TensorBoard</code> to monitor the model and ensure overfitting and underfitting are avoided. <br />
                The model once trained is tested against a test set of images which are randomly obtained through the population. <br />
                I believe the model encompasses a 80%/20% split in terms of training and testing data respectively.  </p>
                
                <p>The training and testing set of images are converted to pickle files and are then shuffled to ensure the order is random. <br />
                This is important to ensure the model does not overtrain on one particular characteristic and has an fair chance of getting any <br />
                of the 4 labelled classes.  </p>
                
                <p>The model itself is then saved as a .h5 file using <code>h5.py</code> and then subsequently, as .json files for easy access and easy loading/reading. <br />
                These files take up a lot of space so they are generally hidden via <code>.gitignore</code>.</p>
                
                <h5 id="modelaccuracyandmodelperformance">Model Accuracy and Model Performance</h5>
                
                <p><strong>Fig 1.0 - Early Stage Model - Accuracy: 51% - Tweaking at a minimum of model parameters</strong>
                <img src="{% static 'application/resources/blog/images/early-stage-accuracy-machine-learning-model_(relu.PNG' %}" alt="Initial Model Performance"/> </p>
                
                <p>At this point, the model has not been tuned at all, it simply has been trained using our training images and tested via the test set.
                Some of the details of the network configuration:</p>
                
                <ul>
                <li>The optimizer used is the <code>adam</code> optimizer,</li>
                
                <li>The lost function used is <code>Sparse Categorical Crossentropy</code></li>
                
                <li>Activation function used is: <code>ReLU</code></li>
                </ul>
                
                <p><strong>Fig 1.1 - Best Model Accuracy Achieved: 74% - Tweaked - 25 Epochs, 12,000 images</strong>
                <img src="{% static 'application/resources/blog/images/sigmoid-accuracy-machine-learning-model_v3_(Best_Accuracy-16px_shift.PNG' %}" alt="Best Model Performance" />  </p>
                
                <p>Some of the details of the network configuration:</p>
                
                <ul>
                <li>The optimizer used is the <code>adam</code> optimizer,</li>
                
                <li>The lost function used is <code>Sparse Categorical Crossentropy</code></li>
                
                <li>Activation function used is: <code>Sigmoid</code></li>
                </ul>
                
                <p><strong>Fig 1.2 - Cross Validation (More Realistic) Model - Accuracy: 62% - 5 Epochs - 28,000 images</strong><br><br>
                <img src="{% static 'application/resources/blog/images/cross-subject-validation-62%-accuracy.PNG' %}" alt="Cross-Validated Model Performance" />  </p>
                
                <p>Neural network configuration details are the same as the 'Best Model' described above. However, the test set used has become
                more realistic. Rather than taking 20% of the each class of image and using that as a test set, we have decided to take an entire
                subject's data for Walk, Run, Slow Resistance Cycle and High Resistance Cycle. </p>
                
                <h4 id="whydidwedothis">Why did we do this?</h4>
                
                <p>We felt the individual characteristics that each person has when it comes to human movement must be considered when testing our
                model to get an accurate view. In the ultimatum of this projects life-cycle, the model must be capable of individual differences that <br />
                would generally be associated with each unique person.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost13websitedevelopmenthomepage04032019">Blog Post #13 | Website Development - Home Page | 04/03/2019</h1>
            </div>
            <div class="blog-post">
                <p>The website design uses a simple box model. It's primary goal is to simply host the download for the desktop application, however
                I have decided to incorporate several different sections to understand the concept of the project as well as document any research
                findings.</p>
                
                <p>Website Tabular: <span style="color: orange"> Home, Research, Blog, Discussion </span> </p>
                
                <p>The website uses the <strong>Django Web Framework</strong> for the back-end behaviour and <strong>PostGresql</strong> as the database technology. <br />
                Additionally, <strong>NginX</strong> and <strong>Gunicorn</strong> are used for web server functionality with <strong>LetsEncrypt</strong> being used to supply the 
                necessary SSL certificate to protect data and information on the website via encryption.</p> <br>
                
                <p><strong>Fig 2.0 - Top of Home Page</strong> </p>
                <img src="{% static 'application/resources/blog/images/website-home.png' %}" alt="Top of Home Page" />  </p>
                
                <p>A brief overview of the websites aims and what the software should achieve. Arduino based software with a mission to recognize 
                human activity through machine learning.</p> <br>
                                
                <p><strong>Fig 2.1 - Bottom of Home Page</strong></p>
                <img src="{% static 'application/resources/blog/images/website-home-(1).png' %}" alt="Bottom of Home Page" />
                
                <p>A bit about myself as well as a link to my websites and other online profiles/portfolios. Additionally, there is a link
                to my Trello board which was set-up at a later date for following the projects progress.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost14websitedevelopmentdiscussionpage10032019">Blog Post #14 | Website Development - Discussion Page | 10/03/2019</h1>
            </div>

            <div class="blog-post">
                <p>Due to very many assignments DCU has placed on its final year students, project work had to take a back seat. Many of 
                our modules demanded our attention and assignments were on-going constantly. I managed to make a small bit of progress with
                the project by linking the discussion page up to the Postgres database and enabled the ability to create comments and see
                other folks comments through the front-end UI.</p>
                
                <p><strong>Fig 3.0 - Top of Discussion Page</strong></p>
                <img src="{% static 'application/resources/blog/images/Discussion-page.png' %}" alt="Top of Discussion Page" />
                
                <p>View other users comments about Human Activity Recognition</p>
                
                <hr />
                
                <p><strong>Fig 3.1 - Bottom of Discussion Page</strong> </p>
                <img src="{% static 'application/resources/blog/images/Discussion-page-(1).png' %}" alt="Bottom of Discussion Page" />
                
                <p>The ability to create your own comment and join the discussion.</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost15concurrenttrelloboard11032019">Blog Post #15 | Concurrent Trello Board |  11/03/2019</h1>
            </div>

            <div class="blog-post">
                <p>I set up a Trello Board to increase the connectivity and communication between myself and my supervisor. This allowed us to
                share what activities I was prioritising and what steps needed to be taken to gradually move towards the end-goal. Additionally, 
                the trello project board helped keep me focused on what task was to be ticked off next.  </p>
                
                <p><strong>Fig 4.0 - Trello Project Board</strong> </p>
                <img src="{% static 'application/resources/blog/images/project-progress.png' %}" alt="Trello Board" />
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost16desktopapplicationdesignprocess22032019">Blog Post #16 | Desktop Application Design Process | 22/03/2019</h1>
            </div>

            <div class="blog-post">
                <p>With the Concurrent and Distributed Assignment out of the way as well as the Image Processing assignment, only 1 college assignment was remaining. Thus, my focus was beginning to return back to the fourth year project. I had initalised a basic desktop application GUI
                several weeks back but it was very much in a bare-bones state. The previous iteration of the desktop application was about me understanding
                how to build a desktop application using Python via: <code>PyQt5</code> and <code>PyInstaller</code>.  </p>
                
                <p><code>PyQt5</code> is a special python library for developing desktop applications. Originally based on C++, it was ported over to python enabling
                more flexibility across programming languages.</p>
                
                <p><code>PyInstaller</code> is a python library which is capable of converting a python program to a .exe executable which will be extremely useful 
                when allowing users to download the project from the online platform: https://www.projectactivityrecognition.ml/</p>
                
                <p><strong>Fig 5.0 - Desktop Application Initial Iteration</strong> </p>
                <img src="{% static 'application/resources/blog/images/desktop-app-visual.PNG' %}" alt="Desktop App Iteration #1" />
                
                <p>The initial iteration of the application is somewhat bland, but the idea is there.</p>
                
                <p><strong>Fig 5.1 - Desktop Application Second Iteration</strong> </p>
                <img src="{% static 'application/resources/blog/images/desktop-app-visual-v2.0.PNG' %}" alt="Desktop App Iteration #2" />
                
                <p>The next iteration removed the placeholder calender object in the 2nd window-pane and modified some design aspects.</p>
                
                <p><strong>Fig 5.2 -Desktop Application Third Iteration</strong></p>
                <img src="{% static 'application/resources/blog/images/desktop-app-visual-v3.0.PNG' %}" alt="Desktop App Iteration #3" />
                
                <p>This iteration added the menu bar common in many desktop applications. Additionally the <code>Simulate Activity Recognition</code> button
                found in the 4th window-pane works correctly. Once selected, the file explorer will be brought up on Windows or Linux allowing you
                to select a .csv file for upload. This .csv file must contain a <code>Timestamp</code> column and a <code>Microvolt</code> column for accurate ppg real-
                time playback. Once uploaded, sections of the data will be converted to an image and these images can be passed into our model for
                predictions on which class label they belong to (Walk, Run, Low Resistance Cycle, High Resistance Cycle).</p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost17mqttmosquittosubscribepublishdesignpattern25032019">Blog Post #17 | MQTT &amp; Mosquitto - Subscribe/Publish Design Pattern | 25/03/2019</h1>
            </div>

            <div class="blog-post">
                <p>After speaking with my supervisor at our most recent meeting, we decided to place a large emphasis on the transfer learning portion of
                the project. This will be done using <strong>Google's Inception</strong> https://medium.com/@williamkoehrsen/object-recognition-with-googles-convolutional-neural-networks-2fe65657ff90().  </p>
                
                <p>Additionally, as per standard practice, I was advised to do K-fold cross validation. This involves repeatedly training the model on the different test data, picking a different test subject every time, and ultimately averaging out the 10 test cases. I was also tasked with doing up a Confusion Matrix for CNN results analysis.</p>
                
                <p>In regards to how the <code>Simulate Activity Recognition</code> functionality was, at least from the previous blog post, supposed to work, the architecture has been modified somewhat. The intention now, through discussing with Tomas, is to use the MQTT (Message Queueing Telemetry Transport) protocol to asynchronously perform this function.</p>
                
                <p><strong>How will this be done?</strong></p>
                
                <p>There is a good diagram I created using Balsamiq below which gives a good overview of how all the components connect together. Regardless, 
                I will describe how it works in text. The previous blog posts suggests that images will be generated from the data submitted and then simply
                passed to the machine learning model for a predictive result. However this will no longer be the case. These generated images from the uploaded data will be subsequently compressed. The compressed images will not hinder the machine learning model's prediction as the images are only made up of 64x64 pixels of colours. It is advantageous to use compressed images due to speeding up latency and network performance. The compressed images will then be converted to an array of bytes and can then be sent along the network.  </p>
                
                <p>All of the client machines that download the application will be able to click the particular button and upload a .csv file of the data.
                These client machines that start the application will automatically subscribe to a <code>Open source MQTT broker</code> which I built using <code>Mosquitto</code>. These client machines will also publish data to the broker, that data being the compressed set of images representing the data they uploaded.
                Once uploaded, this set of compressed images will pass to the broker at a particular topic, perhaps for example: <code>data_passageway_topic</code>.  </p>
                
                <p>Another machine on the other side of the network will be subscribed to this topic and will receive the data bytes representative of the image.
                This machine is where the processing will be done, and where the convolutional neural network will be based. It is here that the prediction 
                result will be generated. Once generated, the result will be based back to the client machine and can be coordinated with the software to
                showcase what activity was being performed at that time.</p>
                
                <p>This is extremely advantageous as the program can still continue to perform other processes while asynchronously, the simulated activity is
                being carried out.  </p>
                
                <p><strong>Fig 6.0 - MQTT protocol (Publish / Subscribe)</strong></p>
                <img src="{% static 'application/resources/blog/images/mqtt-model-concept.png' %}" alt="MQTT Balsamiq Diagram" />
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost18machinelearningmodelprogress100x10030032019">Blog Post #18 | Machine Learning Model Progress (100x100) | 30/03/2019</h1>
            </div>

            <div class="blog-pots">
                <p>The machine learning model finally gave a satisfactory accuracy evaluation but it was done using image sizes of 100x100 with a shift period of 16 pixels.
                Walk, Run and Low resistance Bike gave a healthy accuracy evaluation and prediction was roughly 80% accurate on new training data. However, with High 
                Resistance Bike, the model often associated it with Low Resistance Bike or Running. We believe this is due to the lack of data compared to the other classes.
                A conceptual diagram has been posted below to showcase the data variations.</p>
                
                <p>If this problem cannot be solved come the project ultimatum, we may simply just strip High Resistance Bike out of the individual classes,
                leaving us with <strong>Walk, Run and Low Resistance Bike</strong>.</p>
            </div>
            
            <div class="blog-post-title">
                <h1 id="blogpost19mqttmosquittosubscribepublishdesignpattern05042019">Blog Post #19 | MQTT &amp; Mosquitto - Subscribe/Publish Design Pattern | 05/04/2019</h1>
            </div>

            <div class="blog-post">
                <p>The publish-Subscribe model suggested by my Supervisor has been incorporated into the project. This model is based off
                MQTT (Message-Queuing-Telemetry-Transport) which I have mentioned in a previous blog post - More Details on MQTT can be found
                here: https://pypi.org/project/paho-mqtt/ or here: http://mqtt.org/.  </p>
                
                <p>The model is as described above but I will place the concept diagram again here for those interested: </p>
                <img src="{% static 'application/resources/blog/images/mqtt-model-concept.png' %}" alt="MQTT Balsamiq Diagram" />
                
                <p>The Server is always in a looping state and always listening to the broker for requests on particular topics.
                When a client joins, the instantly connects to the broker and subscribe to particular topics, for example:  </p>
                
                <h6 id="topicprediction_receieve">Topic: prediction_receieve</h6>
                
                <p>Additionally, the client also publishes to a topic that the server must subscribe and listen to. When the client activates the <strong>Playback Activity Recognition</strong>
                or <strong>Simulate Activity Recongition</strong> feature, where by they submit a CSV file of PPG signals and the feature can decoded what activity they were performing at that
                given time, the client will post to the server a <strong>Base64</strong> encoded string of SAX (Symbolic Aggregate Approximation) characters.  </p>
                
                <h5 id="whydowesendonlythesaxencodedstringusingbase64">Why do we send only the SAX encoded string using Base64?</h5>
                
                <p>My consideration of this idea led me to building a thin-client / fat-server architecture. The client side is only required to convert the uploaded CSV file of
                PPG signals into a semantic string of symbolic characters. These string representations shows the heart-rate signals which have noise in them. It is from this noise
                we can learn about what activity a person is doing. So, once the string is generated, we encode it using base64 and publish it over the network to our broker (Mosquitto Broker).
                The broker then propagates this message to any clients that are subscribed to that particular topic | IE Our processing server.  </p>
                
                <p>Once the server obtains the message contained the SAX string, we decode it and now have access to the characters representing the time-series data! </p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost20humanactivityrecognitionppgsignalplaybackfeature07042019">Blog Post #20 | Human Activity Recognition PPG signal Playback Feature | 07/04/2019</h1>
            </div>

            <div class="blog-post">
                <p>In the previous blog post, I mentioned we now were able to process the data on a separate server using MQTT. Well, after a few days of brainstorming and
                solving problems and sub-problems, the <strong>Activity Recognition Playback</strong> feature has now been incorporated into the project. </p>
                
                <h5 id="howdoesitwork">How does it work?</h5>
                
                <p>A client machine is capable of submitting a CSV file of PPG signals, normally recorded in micro-volts(mV). We convert this CSV file into a string
                of characters representing the heart-rate signals, minus the inertial sensor of course. This string is encoded using base64 and sent to the processing
                server via MQTT. The server reads the data and selects a substring of the data, and performs <strong>batch processing</strong> on the data such that the client does 
                not have to wait a very long time for the playback feature to become apparent. Without this batch processing style for the feature, the user would be 
                waiting for the entire csv to be converted to a set of bitmap images, and then for each image to be fed into the machine learning model (Convolutional Neural Network)
                before publishing back to the client. This has a strong dependency on the size of the csv file they upload so it would be O(N) in some kind of time-complexity.
                But I removed this O(N) time-complexity and modified it to be constant time, such that only 20 images are decoded at a time and fed into the network.  </p>
                
                <p>The result of the images | IE the prediction that the model determines is published back to the relevant client. The server does this by having a mapping (Python Dictionary)
                from each client object to each client ID that connected. The desktop application which acts as the client is entirely asynchronous in relation to the publishing 
                mechanism going on in the background.  </p>
                
                <p>To facilitate this back-end functionality that I implemented, I incorporated more front-end elements into the desktop application. For example:</p>
                
                <li>1. Inclusion of Activity Animations representing the 4 Activities and an Animation for an 'idle' state.</li>
                <li>2. I have added playback details by-way-of text in the relevant window pane inside the desktop application. </li>
                <li>3. Activity Details which tell what a person is doing (Walk/Run/Low Resistance Bike/High Resistance Bike)</li>
                <li>4. Animations were refactored using image editing tools to correctly fit the window of the application. </li>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost21desktopapplicationupdatev4009042019">Blog Post #21 | Desktop Application Update v4.0 | 09/04/2019</h1>
            </div>

            <div class="blog-post">
                <p>Recent changes among the desktop application design include modifying the <strong>Activity Recognition Playback</strong> functionality to be green in colour with the
                ability to 'disable' once the playback function is happening. We don't want more than one playback to be happening or we could overload the server.
                Each time the server receives an activity playback request, it spins up a new thread to facilitate that client on. The design of the button also now
                includes an icon for an enhanced look.</p>
                
                <p>I have also added a red cross icon animation alongside some text which tells the user if their PPG device is successfully connected.
                Additionally, there is a loading animation that spawns once the activity recognition playback feature is invoked.</p>
                
                <p>Diagram of the initial application state:</p>
                <img src="{% static 'application/resources/blog/images/desktop-app-visual-v4.0.PNG' %}" alt="Desktop Application Version 4.0" />
                
                <p>Diagram of the application during playback state:</p>
                <img src="{% static 'application/resources/blog/images/desktop-app-visual-playback-v4.0.PNG' %}" alt="Desktop Application Version 4.0" />
                
                <h5 id="asyoucanseetheplaybackbuttonhaschangedcolourtoalightgreyindicatingthattheactivityplaybackfeatureisunderway">As you can see, the playback button has changed colour to a light-grey, indicating that the activity playback feature is underway.</h5>
                
                <h5 id="therealsoistheloadinganimationdisplayedbythe3greendotswhichalternateincolourprovidingaloadingimpression">There also is the loading animation, displayed by the 3 green-dots, which alternate in colour, providing a 'loading' impression.</h5>
                
                <h5 id="alsoasyoucanseetheactivityanimationhaschangedaswellsathedetailsassociatedwiththatactivity">Also, as you can see the activity animation has changed, as well sa the details associated with that activity.</h5>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost22machinelearningmodelupdate32x3225032019">Blog Post #22 | Machine Learning Model Update (32 x 32) | 25/03/2019</h1>
            </div>

            <div class="blog-post">
                <p>With recent adaptions to the project, I've modified the image size to be be 32x32 that is up-scaled to be 128x128. The reason for this change
                was that, previously with the 100x100 image approach, 100x100 is enough characters to determine 40-seconds of the data file uploaded.
                This is not what we want, we want to be able to determine what activity is being performed after only several seconds of interpretation.
                With 32x32, we can try predict what activity is being performed using only 4-seconds of data. If possible, we can also look at 2-seconds of data
                for optimal behaviour, but I doubt we will achieve this. </p>
            </div>

            <div class="blog-post-title">
                <h1 id="blogpost23obtainingtheppgenabledarduino11042019">Blog Post #23 | Obtaining the PPG-enabled Arduino   | 11/04/2019</h1>
            </div>

            <div class="blog-post">
                <p>The arduino kit and PPG device alongside a pre-amp have been obtained. Through interacting and communication with members from DCU's Insight Centre,
                I now have the device in question that can perform heart-rate determination using optical signals. The next steps are to incorporate it into the application
                and have the application recognise when a PPG has been connected. My current thinking is that we will have two states for the application, one where
                the application is in an 'idle' mode where the playback functionality can be accessed and other information can be viewed and another state, namely the
                'Real Time Activity Recognition state', where by the connected ppg device's data will be sent to our MQTT broker and subsequently server, and eventually back to
                the client, to determine there activity. When we say 'Real Time activity recognition', we really mean 'Quasi' real time activity recognition, as we will require
                a certain number of seconds to determine their activity | IE 2, 4 or maybe even 8 seconds of data before activity determination.</p>
            </div>

            <!-- Updates to Blog Go Here -->
            <div class="blog-post-title">
                <h1><a id="Blog_Post_24__Graph_Pane_Update__24042019_0"></a>Blog Post #24 | Graph Pane Update | 24/04/2019</h1>
            </div>

            <div class="blog-post">
                <p>Previously, the graph pane of the desktop application UI was just a simple gif animation to provide the ‘feel’ of an active PPG connection.<br>
                However, with recent modifications and updates to the software, I have built in a real-time graph system that can sense when a ppg is connected<br>
                via an arduino connection. The real-time graph system has been built in using Matplotlib and Seaborn.<br><br>
                <img src="{% static 'application/resources/blog/images/modern-overview.PNG' %}" alt="Desktop Application Version 4.0"></p>
            </div>
            
            <div class="blog-post-title">
                <h1><a id="Blog_Post_25__Update_Research_tab__25042019_6"></a>Blog Post #25 | Update Research tab | 25/04/2019</h1>
            </div>
            <div class="blog-post">
                <p>The research tab has been changed into documentation rather than a scroll-able text frame as planned previously. This is primarily due to the fact<br>
                that the bulk of the research will be documented online on the website (<a href="https://projectactivityrecognition.ml">https://projectactivityrecognition.ml</a>). Additionally, in order to facilitate<br>
                time constraints, the decision to update the research pane to be as such seems like a wise decision.<br><br>

                <img src="{% static 'application/resources/blog/images/modern-overview.PNG' %}" alt="Desktop Application Version 4.0"></p>
            </div>
                
            <div class="blog-post-title">
                <h1><a id="Blog_Post_26__Small_Website_update__27042019_12"></a>Blog Post #26 | Small Website update | 27/04/2019</h1>
            </div>

            <div class="blog-post">
                <p>The website got a small update in order to complete the design. Before your opinions come about, be sure to know that a lot<br>
                of the individual components are unfinished.</p>

                <p>The research tab had been split into a grid using <code>CSS Grids</code>. Each of the 4 grid tabs describe each technique the project is based around and<br>
                offers hyperlinks to relevant documentation of said techniques.<br>

                <strong>Note:</strong> Some of the text needs to be modified as it is simply actings as ‘Lorem Ipsum’ text currently.<br><br>
                <img src="{% static 'application/resources/blog/images/modern-web-research.PNG' %}" alt="Desktop Application Version 4.0"></p>

                <p>The blog you are reading now is the one shown in the picture below. However, the blog has been selectively refined for view on the web.<br><br>
                <img src="{% static 'application/resources/blog/images/modern-web-blog.PNG' %}" alt="Desktop Application Version 4.0"></p>

                <p>Finally, the discussion tab has not changed bar the colour scheme update which I have decided is a dark theme. The entire website will have this<br>
                dark theme to match accordingly.<br><br>

                <img src="{% static 'application/resources/blog/images/modern-web-discussion.PNG' %}" alt="Desktop Application Version 4.0"></p>
            </div>

            <div class="blog-post-title">
                <h1><a id="Blog_Post_27__Button_Modifications__28042019_28"></a>Blog Post #27 | Button Modifications | 28/04/2019</h1>
            </div>

            <div class="blog-post">
                <p>Each of the buttons in the controller pane in the overview tab each represent a function. Once selected, it is important to adjust the enable/disable<br>
                property and/or the colour of these buttons to reflect what pushing the buttons do and also how to stop a function currently active.<br>
                For example, if you select the ‘playback activity recognition’ button feature, the button will become grey and be disabled. The button can only<br>
                be re-enabled via either the completion of the function currently carrying out or by selecting the newly highlighted red button to ‘stop recognition playback’.<br><br>
                <img src="{% static 'application/resources/blog/images/modern-overview.PNG' %}" alt="Desktop Application Version 4.0"></p>
            </div>

            <div class="blog-post-title">
                <h1><a id="Blog_Post_28__Tab_View_Update_for_Desktop_Application_UI__30042019_35"></a>Blog Post #28 | Tab View Update for Desktop Application UI | 30/04/2019</h1>
            </div>

            <div class="blog-post">
                <p>The Desktop application has dramatically improved in terms of UI and flexibility via the newly introduced <strong>‘Tab View’</strong>.<br>
                I felt the tab view, with the 4 respective tabs at the top seemed like a good choice to further expand the desktop application,<br>
                and offer different views of different functions. The idea is that the overview tab is a simplified version of the other tabs, basically<br>
                a summary of each. But selecting each individual tab will provide you with a more advanced look at that particular function. With exception to<br>
                the overview tab, I plan to introduce more features for each tab view in the future.</p>

                <h4><a id="Overview_Tab_42"></a>Overview Tab</h4>
                <p><img src="{% static 'application/resources/blog/images/modern-overview.PNG' %}" alt="Desktop Application Version 4.0"></p><br>
                <h4><a id="Activity_Recognition_Tab_44"></a>Activity Recognition Tab</h4>
                <p><img src="{% static 'application/resources/blog/images/modern-activity-pane.PNG' %}" alt="Desktop Application Version 4.0"></p><br>
                <h4><a id="Graph_Tab_46"></a>Graph Tab</h4>
                <p><img src="{% static 'application/resources/blog/images/modern-graph-pane.PNG' %}" alt="Desktop Application Version 4.0"></p><br>
                <h4><a id="Research_Tab_48"></a>Research Tab</h4>
                <p><img src="{% static 'application/resources/blog/images/modern-research-pane.PNG' %}" alt="Desktop Application Version 4.0"></p><br>

                <p>Implementing this functionality into the application caused me a lot of trouble as QT, the desktop framework I’m using to build the application,<br>
                does not directly support widgets being transacted in multiple locations. To overcome my problem, I had to create separate instances of the same object<br>
                and programmatically allocate their view and data based on the overview.</p>
                </div>

            <div class="blog-post-title">
                <h1><a id="Blog_Post_29__RealTime_Activity_Recognition_80_Done__01052019_55"></a>Blog Post #29 | Real-Time Activity Recognition 80% Done | 01/05/2019</h1>
            </div>

            <div class="blog-post">
                <p>The most impressive and daunting feature of my application is the <code>real-time activity recognition playback with an arduino PPG connection</code>.<br>
                Today, the 01 of May I managed to make a large dent in the progress of the function. It was a difficult process to overcome but for the most<br>
                part I have, with the use of buffers and MQTT pub/sub models, we can send our data from the PPG arduino device across the network, where<br>
                it will arrive at a server to process the data and then  feed it into a trained model (previously discussed), which the result will then be<br>
                permeated back to the client machine, thus updating their UI and offering details about what activity is being performed.</p><br>
                <p><img src="{% static 'application/resources/blog/images/modern-overview.PNG' %}" alt="Desktop Application Version 4.0"></p>
            </div>

            <!-- Next Set of Blog Updates -->
            <div class="blog-post-title">
                <h1><a id="Blog_Post_30__Recording_Mode_Implementation__02052019_0"></a>Blog Post #30 | Recording Mode Implementation | 02/05/2019</h1>
            </div>
            <div class="blog-post">
                <p>I have recently included a <code>Recording Mode</code> feature in the application to allow users to equip a wrist-mounted ppg sensor, and record the<br>
                data to a csv file to a specific directory. This feature is incredible useful for recording microvolt data from particular users. This data<br>
                can then be used from the project standpoint to retrain the model with the newly collected data, perhaps converging on a greater accuracy.<br>
                Additionally, this newly recorded data can be fed into out <code>Activity Playback Feature</code> which will be able to tell what activity the person was doing<br>
                at the the time of the recording session.</p>
            </div>

            <div class="blog-post-title">
                <h1><a id="Blog_Post_30__Progress_Bar_Update__03052019_7"></a>Blog Post #30 | Progress Bar Update | 03/05/2019</h1>
            </div>
            <div class="blog-post">
                <p>The progress bar which has been empty and sitting on the controller pane in the desktop application has finally received an update. Now, when<br>
                the <code>Activity Playback Feature</code> is engaged, the progress bar will update automatically alongside the feature to indicate a sense of progress.<br>
                This was achieved using <code>Slots</code> and <code>Signals</code>, which are communication structures between threads. More information on Slots and Signals can be<br>
                found here: <a href="https://wiki.qt.io/Qt_for_Python_Signals_and_Slots">https://wiki.qt.io/Qt_for_Python_Signals_and_Slots</a>.</p>
            </div>

            <div class="blog-post-title">
                <h1><a id="Blog_Post_31__Website_Deployment__03052019_13"></a>Blog Post #31 | Website Deployment | 03/05/2019</h1>
            </div>
            <div class="blog-post">
                <p>The website has been refined slightly to become more suitable to a production-ready web application. I now will be moving to the deployment step<br>
                of the application where I will be pushing the website online. If you are reading this blog post from GitLab, the website can be found at:<br>
                <a href="https://www.projectactivityrecognition.ml">https://www.projectactivityrecognition.ml</a>. <br> 
                The website has been deployed using Gunicorn and Nginx (Reverse Proxy Web Server) and is being hosted online using a free domain name system hosting service.</p>
            </div>

            <div class="blog-post-title">
                <h1><a id="Blog_Post_32__Refining_the_Machine_Learning_Model_and_Achieving_the_Best_Accuracy_Possible__03052019_19"></a>Blog Post #32 | Refining the Machine Learning Model and Achieving the Best Accuracy Possible | 03/05/2019</h1>
            </div>
            <div class="blog-post">
                <p>With the current iteration of the machine learning model, the accuracy among selecting which class best fits the data for prediction, is quite low.<br>
                With the remaining time I have left with the project, I hope to achieve a much higher accuracy via changing certain parameters, retraining the model,<br>
                and perhaps recording new data with the <code>Recording Mode Feature</code> I discussed in blog post #30 above.</p>
            </div>

            <div class="blog-post-title">
                <h1><a id="Blog_Post_33__Tests_Tests_and_more_Tests__03052019_24"></a>Blog Post #33 | Tests, Tests, and more Tests | 03/05/2019</h1>
            </div>
            <div class="blog-post">
                <p>Now with the desktop application UI complete, the web application complete and the project<br>
                download feature in-place, my new goal has become testing the software. I have not left testing to the end of the project however.<br>
                I have tested continuously throughout the development life-cycle of the application. I used DCU’s Computing Gitlab CI/CD functionality<br>
                to run test cases repeatedly after each push to my repository on gitlab. It turned out to be a very smart way to notify me whenever any<br>
                bugs were leaked into the code or if any code functionality broke externally.</p>
                <p>Now with the remainder of my time I will be studying for my Final Year Exams and will also be writing <code>Unit and Integration tests</code> to<br>
                achieve as high a test-coverage as possible before the project deadline.</p>
            </div>
        </div>

        <!-- Footer  -->
        
        <script src="" async defer></script>
    </body>
</html>